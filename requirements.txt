# ---------- Core ----------
python-dotenv==1.0.1
PyYAML>=6.0.1,<7
pydantic>=2.8,<3
typer>=0.12.3,<0.13
rich>=13.7,<14
structlog>=24.1,<25

# ---------- Retrieval & embeddings ----------
faiss-cpu>=1.8.0,<2
unstructured[pdf]
pytesseract
pdfminer.six==20250506
pdfminer.six[image]==20250506
pi-heif==1.0.0
sentence-transformers>=3.0.1,<4
numpy>=1.26,<3
pandas>=2.2,<3
tqdm>=4.66,<5
rapidfuzz>=3.9,<4
scikit-learn>=1.5,<2

# Required by sentence-transformers / transformers backends
torch>=2.3.1,<3           # CPU-only is fine; MPS works on Apple Silicon if available

# ---------- Closed model baseline ----------
openai>=1.40.0,<2
tiktoken>=0.7.0,<0.8

# ---------- Transformers backend (Gemma/Mistral/Llama OSS) ----------
transformers>=4.43,<5
accelerate>=0.33,<1
sentencepiece>=0.2.0,<0.3
huggingface_hub>=0.24,<0.26

# ---------- Optional backends (commented for Mac safety) ----------
# bitsandbytes>=0.43,<0.44   # NOT supported on macOS; enable only on Linux/NVIDIA
llama-cpp-python>=0.2.86,<0.3  # GGUF local inference (optional)

# ---------- Dev / lint (optional) ----------
# black>=24.4,<25
# ruff>=0.4,<0.5
pytest>=8.0,<9
pyarrow>=17.0,<18